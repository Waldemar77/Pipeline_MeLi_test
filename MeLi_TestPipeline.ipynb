{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1qr-1HBliaDAAUhejE4Mut_eaFaGNsp9J",
      "authorship_tag": "ABX9TyNTBEDubFNVRtGaus9D1KcJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Waldemar77/Pipeline_MeLi_test/blob/origin/MeLi_TestPipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOVz_u3lzkkj",
        "outputId": "34284b42-59bf-4941-8dcd-11b0d64f4097",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: orjson in /usr/local/lib/python3.10/dist-packages (3.10.6)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "!pip install orjson\n",
        "import pandas as pd\n",
        "import orjson\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading prints.json and taps.json files using pandas and orjson file for nested json lines file. Furthermore, creating a final dataframe to show last week prints."
      ],
      "metadata": {
        "id": "31TP9f7ZO609"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# >>> Input path for prints.json\n",
        "json_base_path = \"/content/drive/MyDrive/MeLi test/nested_json_files\"\n",
        "\n",
        "# Initialize an empty list to store processed rows\n",
        "rows = []\n",
        "df_whole = pd.DataFrame()\n",
        "df_prints = pd.DataFrame()\n",
        "df_taps = pd.DataFrame()\n",
        "\n",
        "# Iterate over each JSON file in the base path\n",
        "for json_file in os.listdir(json_base_path):\n",
        "\n",
        "  # Construct the full path to the JSON file\n",
        "  json_path = os.path.join(json_base_path, json_file)\n",
        "\n",
        "  # Read and process the JSON lines file iteratively\n",
        "  try:\n",
        "    with open(json_path, 'r') as file:\n",
        "      for line in file:\n",
        "        # Parse each line as JSON using orjson\n",
        "        record = orjson.loads(line)\n",
        "        # Flatten the nested JSON structure\n",
        "        flattened_record = {\n",
        "            'day': record['day'],\n",
        "            'user_id': record['user_id'],\n",
        "            'event_data.position': record['event_data']['position'],\n",
        "            'event_data.value_prop': record['event_data']['value_prop']\n",
        "        }\n",
        "        # Append the flattened record to the list\n",
        "        rows.append(flattened_record)\n",
        "\n",
        "      # Creating dataframe with rows list\n",
        "      temp_df = pd.DataFrame(rows)\n",
        "\n",
        "      # Adding a new column to identify json file\n",
        "      temp_df['event_type'] = f'{json_file}'\n",
        "\n",
        "      # casting to date format\n",
        "      temp_df['day'] = pd.to_datetime(temp_df['day'])\n",
        "\n",
        "      # casting string values to integer format\n",
        "      #temp_df['user_id'] = temp_df['user_id'].astype(int)\n",
        "      #temp_df['event_data.position'] = temp_df['event_data.position'].astype(int)\n",
        "\n",
        "      # Concatenating the temporary DataFrame with the main DataFrame\n",
        "      df_whole = pd.concat([df_whole, temp_df], ignore_index=True)\n",
        "  except Exception as e:\n",
        "    print(f\"Error processing {json_file}: {e}\")\n",
        "\n",
        "# >>> Transforming and creating aux dataframes\n",
        "try:\n",
        "  # Saving taps records\n",
        "  df_taps = df_whole[df_whole['event_type'] == 'taps.json']\n",
        "\n",
        "  # Saving prints records\n",
        "  df_prints = df_whole[df_whole['event_type'] == 'prints.json']\n",
        "\n",
        "  # calculating the first 3 weeks for df_taps and df_prints, calculated with the las day\n",
        "  df_taps = df_taps[df_taps['day'] <= df_taps['day'].max() - pd.Timedelta(days=7)]\n",
        "  df_prints = df_prints[df_prints['day'] <= df_prints['day'].max() - pd.Timedelta(days=7)]\n",
        "\n",
        "  # creating new dataframe with only last week prints, calculated with the last day\n",
        "  df_prints_lastW = df_prints[df_prints['day'] >= df_prints['day'].max() - pd.Timedelta(days=7)]\n",
        "\n",
        "  # dropping column event_data.position on df_prints_lastW\n",
        "  df_prints_lastW = df_prints_lastW.drop('event_data.position', axis=1)\n",
        "\n",
        "  # renaming event_data.value_prop\n",
        "  df_prints_lastW = df_prints_lastW.rename(columns={'event_data.value_prop': 'value_prop'})\n",
        "  df_taps = df_taps.rename(columns={'event_data.value_prop': 'value_prop'})\n",
        "  df_prints = df_prints.rename(columns={'event_data.value_prop': 'value_prop'})\n",
        "\n",
        "  # Calculating for each user_id and value_prop in df_prints_lastW\n",
        "  #how many user_id and value_prop are found in df_taps\n",
        "  df_taps_count = df_taps.groupby(['user_id', 'value_prop']).size().reset_index(name='taps_count')\n",
        "\n",
        "  # Calculating quantity of prints first 3 weeks\n",
        "  df_prints_count = df_prints.groupby(['user_id', 'value_prop']).size().reset_index(name='prints_count')\n",
        "except Exception as e:\n",
        "  print(f\"Error processing ETL: {e}\")"
      ],
      "metadata": {
        "id": "jDpB-RbpFAaL"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importin csv file\n",
        "csv_base_path = '/content/drive/MyDrive/MeLi test/csv_files/'\n",
        "\n",
        "df_payment = pd.DataFrame()\n",
        "\n",
        "# Reading csv file\n",
        "try:\n",
        "  # Iterate over each CSV file in the base path\n",
        "  for csv_file in os.listdir(csv_base_path):\n",
        "\n",
        "    # Construct the full path to the CSV file\n",
        "    if csv_file.endswith('.csv'):\n",
        "      csv_path = os.path.join(csv_base_path, csv_file)\n",
        "      df_payment = pd.read_csv(csv_path, sep=',', encoding='utf-8', header=0)\n",
        "\n",
        "      # Casting date field\n",
        "      df_payment['pay_date'] = pd.to_datetime(df_payment['pay_date'])\n",
        "\n",
        "  # dataframe for quantity of payments by user_id and value_prop\n",
        "  df_payment_qty = df_payment.groupby(['user_id', 'value_prop']).count().reset_index()\n",
        "  df_payment_qty = df_payment_qty[['user_id', 'value_prop', 'pay_date']]\n",
        "  df_payment_qty.rename(columns={'pay_date': 'payment_qty'}, inplace=True)\n",
        "\n",
        "  # dataframe with amount of payments by user_id and value_prop, sumarize by 'total'\n",
        "  df_payment = df_payment.drop('pay_date', axis=1)\n",
        "  df_payment_sum = df_payment.groupby(['user_id', 'value_prop']).sum().reset_index()\n",
        "  df_payment_sum.rename(columns={'total': 'payment_sum'}, inplace=True)\n",
        "except Exception as e:\n",
        "  print(f\"Error processing csv loading and transformation: {e}\")\n",
        "\n",
        "#print(df_payment.info())\n",
        "#print(df_payment_qty.head())\n",
        "#print(df_payment_qty.query('user_id == 25'))\n",
        "#print(df_payment_sum.query('user_id == 25'))\n"
      ],
      "metadata": {
        "id": "H6eZk6El2Qzo"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  # Joining df_prints_lastW with df_prints_count by user_id and value_prop\n",
        "  df_prints_lastW = df_prints_lastW.merge(df_prints_count, on=['user_id', 'value_prop'], how='left')\n",
        "\n",
        "  # Joining df_prints_lastW with df_taps_count by user_id and value_prop\n",
        "  df_prints_lastW = df_prints_lastW.merge(df_taps_count, on=['user_id', 'value_prop'], how='left')\n",
        "\n",
        "  # Joining df_prints_lastW with df_payment_qty by user_id and value_prop\n",
        "  df_prints_lastW = df_prints_lastW.merge(df_payment_qty, on=['user_id', 'value_prop'], how='left')\n",
        "\n",
        "  # Joining df_prints_lastW with df_payment_sum by user_id and event_data.value_prop\n",
        "  df_prints_lastW = df_prints_lastW.merge(df_payment_sum, on=['user_id', 'value_prop'], how='left')\n",
        "\n",
        "  # Filling the missing values with 0\n",
        "  df_prints_lastW.fillna(0, inplace=True)\n",
        "\n",
        "  # Creating a new column with True if taps_count > 0, otherwise, False\n",
        "  df_prints_lastW['has_taps'] = df_prints_lastW['taps_count'] > 0\n",
        "\n",
        "  # Deleting duplicates on value_prop\n",
        "  df_prints_lastW = df_prints_lastW.drop_duplicates(subset=['user_id','value_prop'])\n",
        "  df_prints_lastW = df_prints_lastW.drop('event_type', axis=1)\n",
        "except Exception as e:\n",
        "  print(f\"Error processing joins on definitive dataframe 'df_prints_lastW' and aux dataframes: {e}\")"
      ],
      "metadata": {
        "id": "fg8WzSknPA1W"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(df_prints_lastW.query(\"taps_count > 1\"))\n",
        "#print(df_prints_lastW.query(\"has_taps == True\"))\n",
        "print(df_prints_lastW.query(\"user_id == 25\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5ELloZ-dBnk",
        "outputId": "abc29f89-dbea-4f11-8232-cb5598ba1064"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              day  user_id          value_prop  prints_count  taps_count  \\\n",
            "5131   2020-11-18       25    credits_consumer             2         1.0   \n",
            "5132   2020-11-18       25               point             2         1.0   \n",
            "64558  2020-11-18       25  cellphone_recharge             2         0.0   \n",
            "64560  2020-11-18       25             prepaid             2         0.0   \n",
            "149779 2020-11-23       25          send_money             1         0.0   \n",
            "\n",
            "        payment_qty  payment_sum  has_taps  \n",
            "5131            0.0         0.00      True  \n",
            "5132            2.0       176.72      True  \n",
            "64558           2.0        54.67     False  \n",
            "64560           1.0        27.69     False  \n",
            "149779          2.0        24.36     False  \n"
          ]
        }
      ]
    }
  ]
}